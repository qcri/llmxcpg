{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/khang/anaconda3/envs/vulbert/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "import torchtext.vocab as vocab\n",
    "import sklearn.metrics\n",
    "from transformers import RobertaModel\n",
    "from transformers import RobertaConfig\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "from torch.optim import SGD,Adam,RMSprop\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "from clang import *\n",
    "\n",
    "\n",
    "seed = 1234\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda:3\n",
      "MultiGPU:  False\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "multigpu = False\n",
    "if device == torch.device('cuda'):\n",
    "\tmultigpu = torch.cuda.device_count() > 1\n",
    "print('Device: ',device)\n",
    "print('MultiGPU: ',multigpu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training & vocab parameters\n",
    "DATA_PATH = 'data'\n",
    "VOCAB_SIZE = 50000\n",
    "BATCH_SIZE = 128\n",
    "EMBED_SIZE = VOCAB_SIZE+2\n",
    "EMBED_DIM = 768 #768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenizer\n",
    "\n",
    "from tokenizers.pre_tokenizers import PreTokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers import NormalizedString,PreTokenizedString\n",
    "from typing import List \n",
    "\n",
    "class MyTokenizer:\n",
    "    \n",
    "    cidx = cindex.Index.create()\n",
    "        \n",
    "\n",
    "    def clang_split(self, i: int, normalized_string: NormalizedString) -> List[NormalizedString]:\n",
    "        ## Tokkenize using clang\n",
    "        tok = []\n",
    "        tu = self.cidx.parse('tmp.c',\n",
    "                       args=[''],  \n",
    "                       unsaved_files=[('tmp.c', str(normalized_string.original))],  \n",
    "                       options=0)\n",
    "        for t in tu.get_tokens(extent=tu.cursor.extent):\n",
    "            spelling = t.spelling.strip()\n",
    "            \n",
    "            if spelling == '':\n",
    "                continue\n",
    "                \n",
    "            ## Keyword no need\n",
    "\n",
    "            ## Punctuations no need\n",
    "\n",
    "            ## Literal all to BPE\n",
    "            \n",
    "            #spelling = spelling.replace(' ', '')\n",
    "            tok.append(NormalizedString(spelling))\n",
    "\n",
    "        return(tok)\n",
    "    \n",
    "    def pre_tokenize(self, pretok: PreTokenizedString):\n",
    "        pretok.split(self.clang_split)\n",
    "        \n",
    "## Custom tokenizer\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers import normalizers,decoders\n",
    "from tokenizers.normalizers import StripAccents, unicode_normalizer_from_str, Replace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers import processors,pre_tokenizers\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "## Load pre-trained tokenizers\n",
    "vocab, merges = BPE.read_file(vocab=\"./tokenizer/drapgh-vocab.json\", merges=\"./tokenizer/drapgh-merges.txt\")\n",
    "my_tokenizer = Tokenizer(BPE(vocab, merges, unk_token=\"<unk>\"))\n",
    "\n",
    "my_tokenizer.normalizer = normalizers.Sequence([StripAccents(), Replace(\" \", \"Ã„\")])\n",
    "my_tokenizer.pre_tokenizer = PreTokenizer.custom(MyTokenizer())\n",
    "my_tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "my_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"<s> $A </s>\",\n",
    "    special_tokens=[\n",
    "    (\"<s>\",0),\n",
    "    (\"<pad>\",1),\n",
    "    (\"</s>\",2),\n",
    "    (\"<unk>\",3),\n",
    "    (\"<mask>\",4)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPARE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_ONLY = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydataset = 'devign'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer.enable_truncation(max_length=1024)\n",
    "my_tokenizer.enable_padding(direction='right', pad_id=1, pad_type_id=0, pad_token='<pad>', length=None, pad_to_multiple_of=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(code):\n",
    "    ## Remove code comments\n",
    "    pat = re.compile(r'(/\\*([^*]|(\\*+[^*/]))*\\*+/)|(//.*)')\n",
    "    code = re.sub(pat,'',code)\n",
    "    code = re.sub('\\n','',code)\n",
    "    code = re.sub('\\t','',code)\n",
    "    return(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_encodings(encodings):\n",
    "    input_ids=[]\n",
    "    attention_mask=[]\n",
    "    for enc in encodings:\n",
    "        input_ids.append(enc.ids)\n",
    "        attention_mask.append(enc.attention_mask)\n",
    "    return {'input_ids':input_ids, 'attention_mask':attention_mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_colname(x):\n",
    "    try:\n",
    "        x = x.rename(columns={'functionSource': \"func\"})\n",
    "    except:\n",
    "        None\n",
    "\n",
    "    try:\n",
    "        x = x.rename(columns={'code': \"func\"})\n",
    "    except:\n",
    "        None\n",
    "\n",
    "    try:\n",
    "        x = x.rename(columns={'label': \"target\"})\n",
    "    except:\n",
    "        None\n",
    "    return(x)\n",
    "\n",
    "if mydataset =='devign':\n",
    "    if TEST_ONLY:\n",
    "        \n",
    "        # test_index=set()\n",
    "        # with open('data/finetune/devign/test.txt') as f:\n",
    "        #     for line in f:\n",
    "        #         line=line.strip()\n",
    "        #         test_index.add(int(line))\n",
    "        mydata = pd.read_json('./data/sven.jsonl', lines=True)\n",
    "        m3=mydata.copy()\n",
    "        mydata = None\n",
    "        del(mydata)\n",
    "        \n",
    "    else:\n",
    "        train_index=set()\n",
    "        valid_index=set()\n",
    "        test_index=set()\n",
    "\n",
    "        with open('data/finetune/devign/train.txt') as f:\n",
    "            for line in f:\n",
    "                line=line.strip()\n",
    "                train_index.add(int(line))\n",
    "\n",
    "        with open('data/finetune/devign/valid.txt') as f:\n",
    "            for line in f:\n",
    "                line=line.strip()\n",
    "                valid_index.add(int(line))\n",
    "\n",
    "        with open('data/finetune/devign/test.txt') as f:\n",
    "            for line in f:\n",
    "                line=line.strip()\n",
    "                test_index.add(int(line))\n",
    "\n",
    "        mydata = pd.read_json('data/finetune/devign/Devign.json')\n",
    "        m1=mydata.iloc[list(train_index)]\n",
    "        m2=mydata.iloc[list(valid_index)]\n",
    "        m3=mydata.iloc[list(test_index)]\n",
    "\n",
    "        mydata = None\n",
    "        del(mydata)\n",
    "    \n",
    "\n",
    "elif mydataset =='d2a':\n",
    "    task = 'function'\n",
    "    \n",
    "    if TEST_ONLY:\n",
    "        m3 = pd.read_csv('data/finetune/%s/%s/d2a_lbv1_%s_dev.csv'%(mydataset,task,task))\n",
    "        m3 = replace_colname(m3)\n",
    "    else:\n",
    "        m1 = pd.read_csv('data/finetune/%s/%s/d2a_lbv1_%s_train.csv'%(mydataset,task,task))\n",
    "        m2 = pd.read_csv('data/finetune/%s/%s/d2a_lbv1_%s_dev.csv'%(mydataset,task,task))\n",
    "        m3 = pd.read_csv('data/finetune/%s/%s/d2a_lbv1_%s_test.csv'%(mydataset,task,task))\n",
    "       \n",
    "        m1 = replace_colname(m1)\n",
    "        m2 = replace_colname(m2)\n",
    "        m3 = replace_colname(m3)\n",
    "        \n",
    "        \n",
    "else:\n",
    "    \n",
    "    def replace_colname(x):\n",
    "        try:\n",
    "            x = x.rename(columns={'functionSource': \"func\"})\n",
    "        except:\n",
    "            None\n",
    "            \n",
    "        try:\n",
    "            x = x.rename(columns={'code': \"func\"})\n",
    "        except:\n",
    "            None\n",
    "\n",
    "        try:\n",
    "            x = x.rename(columns={'label': \"target\"})\n",
    "        except:\n",
    "            None\n",
    "        return(x)\n",
    "    \n",
    "    \n",
    "    if TEST_ONLY:\n",
    "        m3 = pd.read_pickle('data/finetune/%s/%s_test.pkl'%(mydataset,mydataset))\n",
    "        m3 = replace_colname(m3)\n",
    "        \n",
    "    else:\n",
    "        m1 = pd.read_pickle('data/finetune/%s/%s_train.pkl'%(mydataset,mydataset))\n",
    "        m2 = pd.read_pickle('data/finetune/%s/%s_val.pkl'%(mydataset,mydataset))\n",
    "        m3 = pd.read_pickle('data/finetune/%s/%s_test.pkl'%(mydataset,mydataset))\n",
    "\n",
    "        m1 = replace_colname(m1)\n",
    "        m2 = replace_colname(m2)\n",
    "        m3 = replace_colname(m3)\n",
    "\n",
    "if TEST_ONLY:\n",
    "    m3.func = m3.func.apply(cleaner)\n",
    "    test_encodings = my_tokenizer.encode_batch(m3.func)\n",
    "    try:\n",
    "        test_encodings = [{'func':enc.ids,'target':lab} for enc,lab in zip(test_encodings,m3.target.tolist())]\n",
    "    except:\n",
    "        test_encodings = [{'func':enc.ids,'target':lab} for enc,lab in zip(test_encodings,(m3['combine']*1).tolist())]\n",
    "\n",
    "else:\n",
    "    \n",
    "    m1.func = m1.func.apply(cleaner)\n",
    "    train_encodings = my_tokenizer.encode_batch(m1.func)\n",
    "    try:\n",
    "        train_encodings = [{'func':enc.ids,'target':lab} for enc,lab in zip(train_encodings,m1.target.tolist())]\n",
    "    except:\n",
    "        train_encodings = [{'func':enc.ids,'target':lab} for enc,lab in zip(train_encodings,(m1['combine']*1).tolist())]\n",
    "\n",
    "\n",
    "    m2.func = m2.func.apply(cleaner)\n",
    "    val_encodings = my_tokenizer.encode_batch(m2.func)\n",
    "    try:\n",
    "        val_encodings = [{'func':enc.ids,'target':lab} for enc,lab in zip(val_encodings,m2.target.tolist())]\n",
    "    except:\n",
    "        val_encodings = [{'func':enc.ids,'target':lab} for enc,lab in zip(val_encodings,(m2['combine']*1).tolist())]\n",
    "\n",
    "        \n",
    "    m3.func = m3.func.apply(cleaner)\n",
    "    test_encodings = my_tokenizer.encode_batch(m3.func)\n",
    "    try:\n",
    "        test_encodings = [{'func':enc.ids,'target':lab} for enc,lab in zip(test_encodings,m3.target.tolist())]\n",
    "    except:\n",
    "        test_encodings = [{'func':enc.ids,'target':lab} for enc,lab in zip(test_encodings,(m3['combine']*1).tolist())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODES = torchtext.data.Field(batch_first=True, fix_length=1024,use_vocab=False)\n",
    "LABEL = torchtext.data.LabelField(dtype=torch.long, is_target=True,use_vocab=False)\n",
    "fields = {'func': ('codes', CODES), 'target': ('label', LABEL)}\n",
    "\n",
    "class TabularDataset_From_List(torchtext.data.Dataset):\n",
    "    def __init__(self, input_list, format, fields, skip_header=False, **kwargs):\n",
    "        make_example = {\n",
    "            'json': torchtext.data.Example.fromJSON, 'dict': torchtext.data.Example.fromdict}[format.lower()]\n",
    "\n",
    "        examples = [make_example(item, fields) for item in input_list]\n",
    "\n",
    "        if make_example in (torchtext.data.Example.fromdict, torchtext.data.Example.fromJSON):\n",
    "            fields, field_dict = [], fields\n",
    "            for field in field_dict.values():\n",
    "                if isinstance(field, list):\n",
    "                    fields.extend(field)\n",
    "                else:\n",
    "                    fields.append(field)\n",
    "\n",
    "        super(TabularDataset_From_List, self).__init__(examples, fields, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, path=None, root='.data', train=None, validation=None,\n",
    "               test=None, **kwargs):\n",
    "        if path is None:\n",
    "            path = cls.download(root)\n",
    "        train_data = None if train is None else cls(\n",
    "            train, **kwargs)\n",
    "        val_data = None if validation is None else cls(\n",
    "            validation, **kwargs)\n",
    "        test_data = None if test is None else cls(\n",
    "            test, **kwargs)\n",
    "        return tuple(d for d in (train_data, val_data, test_data)\n",
    "                     if d is not None)\n",
    "\n",
    "\n",
    "## Import the 100K data as TabularDataset\n",
    "\n",
    "if TEST_ONLY:\n",
    "    test_data = TabularDataset_From_List(test_encodings,'dict',fields = fields)\n",
    "else:\n",
    "    train_data = TabularDataset_From_List(train_encodings,'dict',fields = fields)\n",
    "    val_data = TabularDataset_From_List(val_encodings,'dict',fields = fields)\n",
    "    test_data = TabularDataset_From_List(test_encodings,'dict',fields = fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IF ITERABLE DATASETTEST_ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(IterableDataset):\n",
    "    \n",
    "    def __init__(self,filename,rcount):\n",
    "     \n",
    "        self.filename=filename\n",
    "        self.len_labels=rcount\n",
    "        super().__init__()\n",
    "                    \n",
    "    def process(self,filename):\n",
    "        import pickle \n",
    "        with open(filename, \"rb\") as f:\n",
    "            while True:\n",
    "                try:\n",
    "                    item = pickle.load(f)\n",
    "                    yield {'input_ids': torch.tensor(item['input_ids']), 'attention_mask':torch.tensor(item['attention_mask']), 'labels':torch.tensor(item['labels'])}\n",
    "                except EOFError:\n",
    "                    break\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return self.len_labels\n",
    "\n",
    "    def __iter__(self):\n",
    "        dataset=self.process(self.filename)          \n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rcount = len(pd.read_pickle('data/draper/draper_train.pkl'))\n",
    "train_dataset = MyDataset('data/draper/draper_stream_train.pkl', train_rcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_rcount = len(pd.read_pickle('data/draper/draper_val.pkl'))\n",
    "val_dataset = MyDataset('data/draper/draper_stream_val.pkl', val_rcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rcount = len(pd.read_pickle('data/draper/draper_test.pkl'))\n",
    "test_dataset = MyDataset('data/draper/draper_stream_test.pkl', test_rcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END ITERABLE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = VOCAB_SIZE\n",
    "\n",
    "# place into iterators\n",
    "\n",
    "if TEST_ONLY:\n",
    "    test_iterator = torchtext.data.BucketIterator(\n",
    "        test_data, \n",
    "        batch_size = 1,\n",
    "        sort = False,\n",
    "        shuffle = False)\n",
    "    \n",
    "else:\n",
    "    train_iterator, valid_iterator, test_iterator = torchtext.data.BucketIterator.splits(\n",
    "        (train_data, val_data, test_data), \n",
    "        batch_size = BATCH_SIZE,\n",
    "        sort = False,\n",
    "        shuffle = False)\n",
    "\n",
    "UNK_IDX = 3\n",
    "PAD_IDX = 1\n",
    "\n",
    "# test_iterator = torchtext.data.BucketIterator(\n",
    "#     test_data, \n",
    "#     batch_size = BATCH_SIZE,\n",
    "#     sort = False,\n",
    "#     shuffle = False)\n",
    "\n",
    "#from torch.utils.data import DataLoader\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset, shuffle=False, batch_size=BATCH_SIZE)\n",
    "# val_dataloader = DataLoader(val_dataset, shuffle=False, batch_size=BATCH_SIZE)\n",
    "# test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define VulBERTa-CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCNN(nn.Module):\n",
    "    def __init__(self, EMBED_SIZE, EMBED_DIM):\n",
    "        super(myCNN,self).__init__()\n",
    "        \n",
    "        pretrained_weights = RobertaModel.from_pretrained('./models/VulBERTa/').embeddings.word_embeddings.weight\n",
    "\n",
    "        self.embed = nn.Embedding.from_pretrained(pretrained_weights,\n",
    "                                                  freeze=True,\n",
    "                                                  padding_idx=1)\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=EMBED_DIM, out_channels=200, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(in_channels=EMBED_DIM, out_channels=200, kernel_size=4)\n",
    "        self.conv3 = nn.Conv1d(in_channels=EMBED_DIM, out_channels=200, kernel_size=5)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc1 = nn.Linear(200*3,256) #500\n",
    "        self.fc2 = nn.Linear(256,128)\n",
    "        self.fc3 = nn.Linear(128,2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = x.permute(0,2,1)\n",
    "\n",
    "        x1 = F.relu(self.conv1(x))\n",
    "        x2 = F.relu(self.conv2(x))\n",
    "        x3 = F.relu(self.conv3(x))\n",
    "        \n",
    "        x1 = F.max_pool1d(x1, x1.shape[2])\n",
    "        x2 = F.max_pool1d(x2, x2.shape[2])\n",
    "        x3 = F.max_pool1d(x3, x3.shape[2])\n",
    "        \n",
    "        x = torch.cat([x1,x2,x3],dim=1)\n",
    "        \n",
    "        # flatten the tensor\n",
    "        x = x.flatten(1)\n",
    "        \n",
    "        # apply mean over the last dimension\n",
    "        #x = torch.mean(x, -1)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return(x)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./models/VulBERTa/ were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ./models/VulBERTa/ and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = myCNN(EMBED_SIZE,EMBED_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.embed.weight.data[UNK_IDX] = torch.zeros(EMBED_DIM)\n",
    "model.embed.weight.data[PAD_IDX] = torch.zeros(EMBED_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myCNN(\n",
      "  (embed): Embedding(50000, 768, padding_idx=1)\n",
      "  (conv1): Conv1d(768, 200, kernel_size=(3,), stride=(1,))\n",
      "  (conv2): Conv1d(768, 200, kernel_size=(4,), stride=(1,))\n",
      "  (conv3): Conv1d(768, 200, kernel_size=(5,), stride=(1,))\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=600, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if multigpu:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of trainable param:  2030810\n"
     ]
    }
   ],
   "source": [
    "print('Num of trainable param: ',sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sklearn\n",
    "\n",
    "try:\n",
    "    cw = sklearn.utils.class_weight.compute_class_weight(class_weight='balanced',classes=[0,1],y=m3.label.tolist())\n",
    "except:\n",
    "    cw = sklearn.utils.class_weight.compute_class_weight(class_weight='balanced',classes=[0,1],y=m3.target.tolist())\n",
    "    \n",
    "c_weights = torch.FloatTensor([cw[0], cw[1]])\n",
    "criterion = nn.CrossEntropyLoss(weight=c_weights)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_accuracy(probs,all_labels):\n",
    "    def getClass(x):\n",
    "        return(x.index(max(x)))\n",
    "    \n",
    "    all_labels = all_labels.tolist()\n",
    "    probs = pd.Series(probs.tolist())\n",
    "    all_predicted = probs.apply(getClass)\n",
    "    all_predicted.reset_index(drop=True, inplace=True)\n",
    "    vc = pd.value_counts(all_predicted == all_labels)\n",
    "    try:\n",
    "        acc = vc[1]/len(all_labels)\n",
    "    except:\n",
    "        if(vc.index[0]==False):\n",
    "            acc = 0\n",
    "        else:\n",
    "            acc = 1\n",
    "    return(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model_foldername = 'VB-CNN_%s'%(mydataset)\n",
    "except FileExistsError:\n",
    "    print('Folder exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training started.....')\n",
    "\n",
    "EPOCHS=20\n",
    "BEST_VAL = 9999.9\n",
    "BEST_MODEL = None\n",
    "BEST_EPOCH = None\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    running_acc = 0\n",
    "    running_loss = 0\n",
    "    timer = time.time()\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_iterator:\n",
    "        batch.codes, batch.label = batch.codes.to(device), batch.label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch.codes)\n",
    "        loss = criterion(output, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = softmax_accuracy(output,batch.label)\n",
    "        running_acc += acc\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        running_acc_val = 0\n",
    "        running_loss_val = 0\n",
    "        for batch in valid_iterator:\n",
    "            batch.codes, batch.label = batch.codes.to(device), batch.label.to(device)\n",
    "            output_val = model(batch.codes)\n",
    "            loss_val = criterion(output_val,batch.label)\n",
    "            acc_val = softmax_accuracy(output_val,batch.label)\n",
    "            running_acc_val += acc_val\n",
    "            running_loss_val += loss_val.item()\n",
    "\n",
    "    print_out = \"Epoch %d - Training acc: %.4f -Training loss: %.4f - Val acc: %.4f - Val loss: %.4f - Time: %.4fs \\n\" % (e+1,\n",
    "    running_acc/len(train_iterator),\n",
    "    running_loss/len(train_iterator),\n",
    "    running_acc_val/len(valid_iterator),\n",
    "    running_loss_val/len(valid_iterator),\n",
    "    (time.time()-timer))\n",
    "    \n",
    "    \n",
    "    selected_model = False\n",
    "    \n",
    "    if selected_model:\n",
    "        \n",
    "        myfile = open(\"res.txt\", \"a\")\n",
    "\n",
    "        if (running_loss_val/len(valid_iterator)) < BEST_VAL:\n",
    "            print('Val_loss decreased!')\n",
    "            print(print_out, end='')\n",
    "            myfile.write('Val_loss decreased!')\n",
    "            myfile.write(print_out)\n",
    "\n",
    "            BEST_VAL = (running_loss_val/len(valid_iterator))\n",
    "            BEST_MODEL = copy.deepcopy(model)\n",
    "            BEST_EPOCH = e+1\n",
    "            model_name = 'models/%s/model_ep_%d.tar' % (model_foldername,e+1)\n",
    "            torch.save({\n",
    "                'epoch': e+1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss}, model_name)\n",
    "\n",
    "        else:\n",
    "            print(print_out, end='')\n",
    "            myfile.write(print_out)\n",
    "\n",
    "        myfile.close()\n",
    "        \n",
    "    else:\n",
    "        print(print_out, end='')\n",
    "        model_name = 'models/%s/model_ep_%d.tar' % (model_foldername,e+1)\n",
    "        torch.save({\n",
    "            'epoch': e+1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss}, model_name)\n",
    "\n",
    "        \n",
    "\n",
    "print('Training completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_testing(all_pred, all_labels):\n",
    "    def getClass(x):\n",
    "        return(x.index(max(x)))\n",
    "\n",
    "    probs = pd.Series(all_pred)\n",
    "    all_predicted = probs.apply(getClass)\n",
    "    all_predicted.reset_index(drop=True, inplace=True)\n",
    "    vc = pd.value_counts(all_predicted == all_labels)\n",
    "\n",
    "    probs2=[]\n",
    "    for x in probs:\n",
    "        probs2.append(x[1])\n",
    "\n",
    "    confusion = sklearn.metrics.confusion_matrix(y_true=all_labels, y_pred=all_predicted)\n",
    "    print('Confusion matrix: \\n',confusion)\n",
    "\n",
    "    try:\n",
    "        tn, fp, fn, tp = confusion.ravel()\n",
    "        print('\\nTP:',tp)\n",
    "        print('FP:',fp)\n",
    "        print('TN:',tn)\n",
    "        print('FN:',fn)\n",
    "\n",
    "        ## Performance measure\n",
    "        print('\\nAccuracy: '+ str(sklearn.metrics.accuracy_score(y_true=all_labels, y_pred=all_predicted)))\n",
    "        print('Precision: '+ str(sklearn.metrics.precision_score(y_true=all_labels, y_pred=all_predicted)))\n",
    "        print('F-measure: '+ str(sklearn.metrics.f1_score(y_true=all_labels, y_pred=all_predicted)))\n",
    "        print('Recall: '+ str(sklearn.metrics.recall_score(y_true=all_labels, y_pred=all_predicted)))\n",
    "        print('Precision-Recall AUC: '+ str(sklearn.metrics.average_precision_score(y_true=all_labels, y_score=probs2)))\n",
    "        print('AUC: '+ str(sklearn.metrics.roc_auc_score(y_true=all_labels, y_score=probs2)))\n",
    "        print('MCC: '+ str(sklearn.metrics.matthews_corrcoef(y_true=all_labels, y_pred=all_predicted)))\n",
    "    except:\n",
    "        None\n",
    "        print('This is multiclass prediction')\n",
    "    return(all_predicted)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.embed.weight\n",
      "module.conv1.weight\n",
      "module.conv1.bias\n",
      "module.conv2.weight\n",
      "module.conv2.bias\n",
      "module.conv3.weight\n",
      "module.conv3.bias\n",
      "module.fc1.weight\n",
      "module.fc1.bias\n",
      "module.fc2.weight\n",
      "module.fc2.bias\n",
      "module.fc3.weight\n",
      "module.fc3.bias\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "for key, val in checkpoint['model_state_dict'].items():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = OrderedDict((key.replace(\"module.\", \"\"), value) for key, value in checkpoint['model_state_dict'].items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('embed.weight',\n",
       "              tensor([[ 0.1219, -0.0006,  0.0046,  ...,  0.0112, -0.0073,  0.0361],\n",
       "                      [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                      [ 0.0320,  0.0168, -0.0243,  ...,  0.0014,  0.0165,  0.0414],\n",
       "                      ...,\n",
       "                      [ 0.1197,  0.0048,  0.0360,  ...,  0.0040,  0.0093, -0.0539],\n",
       "                      [ 0.0986,  0.0185,  0.0002,  ...,  0.0196, -0.0426, -0.0023],\n",
       "                      [ 0.0767,  0.0125,  0.0255,  ..., -0.0072,  0.0289,  0.0235]],\n",
       "                     device='cuda:0')),\n",
       "             ('conv1.weight',\n",
       "              tensor([[[ 0.0241,  0.0720,  0.0911],\n",
       "                       [ 0.1062,  0.1772, -0.2734],\n",
       "                       [-0.1142, -0.1115,  0.2555],\n",
       "                       ...,\n",
       "                       [ 0.0758,  0.0356, -0.0979],\n",
       "                       [ 0.0608,  0.0603, -0.1296],\n",
       "                       [ 0.2610, -0.1467, -0.2619]],\n",
       "              \n",
       "                      [[-0.0422,  0.1145,  0.0630],\n",
       "                       [ 0.1236, -0.0863,  0.0598],\n",
       "                       [-0.0062,  0.2574,  0.0949],\n",
       "                       ...,\n",
       "                       [ 0.0224,  0.1136, -0.1126],\n",
       "                       [ 0.0655, -0.0738, -0.0529],\n",
       "                       [ 0.1303,  0.0893, -0.0643]],\n",
       "              \n",
       "                      [[-0.2645,  0.0734,  0.0767],\n",
       "                       [ 0.3295,  0.0954, -0.2835],\n",
       "                       [ 0.4381, -0.1144, -0.0553],\n",
       "                       ...,\n",
       "                       [ 0.1081,  0.0573, -0.0787],\n",
       "                       [ 0.0970,  0.2229, -0.0181],\n",
       "                       [ 0.0146,  0.1521, -0.0609]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[ 0.1802, -0.0049,  0.0461],\n",
       "                       [ 0.0975,  0.3343,  0.2038],\n",
       "                       [-0.1233, -0.0192,  0.0893],\n",
       "                       ...,\n",
       "                       [ 0.2677,  0.2093,  0.0651],\n",
       "                       [ 0.0788,  0.1986,  0.0137],\n",
       "                       [-0.1283, -0.3604, -0.1895]],\n",
       "              \n",
       "                      [[ 0.0940,  0.0336,  0.1249],\n",
       "                       [ 0.0243,  0.0582,  0.0654],\n",
       "                       [ 0.0499,  0.1536,  0.0475],\n",
       "                       ...,\n",
       "                       [-0.1764,  0.0141,  0.0885],\n",
       "                       [ 0.0206, -0.1721, -0.0069],\n",
       "                       [-0.0302,  0.0480, -0.1325]],\n",
       "              \n",
       "                      [[ 0.0478,  0.0205,  0.0826],\n",
       "                       [ 0.0241,  0.0574, -0.0173],\n",
       "                       [ 0.0474,  0.0393,  0.0585],\n",
       "                       ...,\n",
       "                       [-0.1061, -0.0604, -0.0112],\n",
       "                       [-0.0415,  0.0049, -0.0207],\n",
       "                       [-0.0184,  0.0259, -0.0456]]], device='cuda:0')),\n",
       "             ('conv1.bias',\n",
       "              tensor([-0.2433, -0.3214, -0.3123, -0.0267, -0.3478, -0.3324, -0.2759, -0.2086,\n",
       "                      -0.1624, -0.2173, -0.1510, -0.2357, -0.4431, -0.0418, -0.2956, -0.2723,\n",
       "                      -0.4258, -0.2729, -0.2854, -0.2355, -0.2975, -0.0282, -0.3825, -0.0449,\n",
       "                      -0.4283, -0.3356, -0.1831, -0.3233, -0.1963, -0.1786, -0.3333, -0.3068,\n",
       "                      -0.4120, -0.3997, -0.2944, -0.0667, -0.2632, -0.4317, -0.1979, -0.1763,\n",
       "                      -0.3145, -0.3150, -0.2209, -0.1845, -0.2742, -0.0971, -0.2748, -0.2789,\n",
       "                      -0.0346, -0.4142, -0.1480, -0.2591, -0.1333, -0.1814, -0.1807, -0.2646,\n",
       "                      -0.1933, -0.2403, -0.2122, -0.2954, -0.2591, -0.2049, -0.1445, -0.1569,\n",
       "                      -0.4347, -0.2657, -0.0549, -0.2149, -0.0394, -0.2729, -0.1988, -0.2540,\n",
       "                      -0.4440, -0.2594, -0.2813, -0.3826, -0.3595, -0.4544, -0.2316, -0.1633,\n",
       "                      -0.2100, -0.3713, -0.4488, -0.4491, -0.3144, -0.2285, -0.1444, -0.2659,\n",
       "                      -0.2347, -0.2372, -0.2778, -0.2671, -0.1983, -0.1728, -0.0142, -0.2696,\n",
       "                      -0.2398, -0.1699, -0.2036, -0.3050, -0.4793, -0.2061, -0.3044, -0.1527,\n",
       "                      -0.3841, -0.2231, -0.2060, -0.3128, -0.3561, -0.2845, -0.2576, -0.2030,\n",
       "                      -0.2492, -0.3680, -0.3936, -0.2652, -0.2516, -0.1861, -0.1867, -0.1586,\n",
       "                      -0.3864, -0.1474, -0.2045, -0.3243, -0.2259, -0.1426, -0.2662, -0.1963,\n",
       "                      -0.0252, -0.1861, -0.1259, -0.0390, -0.2797, -0.3280, -0.1876, -0.4596,\n",
       "                      -0.3816, -0.1636, -0.2214, -0.2096, -0.2973, -0.0994, -0.3485, -0.2599,\n",
       "                      -0.3441, -0.2691, -0.1122, -0.2934, -0.2034, -0.3515, -0.2875, -0.1153,\n",
       "                      -0.1607, -0.3972, -0.2402, -0.0519, -0.1697, -0.4520, -0.2859, -0.2651,\n",
       "                      -0.2037, -0.2164, -0.1488, -0.1511, -0.1580, -0.2910, -0.3710, -0.0715,\n",
       "                      -0.1566, -0.3350, -0.0269, -0.1724, -0.3651, -0.0551, -0.1589, -0.1047,\n",
       "                      -0.4054, -0.2810, -0.2068, -0.1900, -0.4553, -0.3575, -0.2170, -0.3586,\n",
       "                      -0.1950, -0.2147, -0.4087, -0.2296, -0.0688, -0.2885, -0.2990, -0.2407,\n",
       "                      -0.2153, -0.2832, -0.2922, -0.2843, -0.2835, -0.2288, -0.2647, -0.1212],\n",
       "                     device='cuda:0')),\n",
       "             ('conv2.weight',\n",
       "              tensor([[[ 8.2751e-03, -3.9140e-02, -1.0493e-01,  7.9428e-02],\n",
       "                       [-1.5180e-01,  4.9376e-02,  1.2827e-01,  2.1209e-01],\n",
       "                       [ 2.0955e-01, -1.2520e-01, -1.3344e-01,  4.1321e-02],\n",
       "                       ...,\n",
       "                       [-1.1224e-01,  1.4639e-01,  1.3369e-01,  2.7840e-01],\n",
       "                       [ 2.7080e-01,  2.6918e-01, -2.4774e-01, -3.0288e-01],\n",
       "                       [-2.8949e-02,  1.3888e-02,  6.9722e-02,  1.1785e-01]],\n",
       "              \n",
       "                      [[ 6.2101e-02,  1.7200e-01, -9.3360e-03,  5.7611e-02],\n",
       "                       [ 2.0848e-01,  1.3907e-01, -1.6352e-01, -3.4254e-02],\n",
       "                       [ 5.3665e-03,  1.6317e-01,  3.5365e-01,  1.0910e-01],\n",
       "                       ...,\n",
       "                       [ 1.8874e-01, -1.1372e-01, -2.5659e-01, -1.5515e-01],\n",
       "                       [ 2.7406e-01, -1.0224e-01,  1.1268e-01,  1.9744e-02],\n",
       "                       [ 1.2452e-01,  3.8766e-02, -2.2883e-02, -2.0471e-01]],\n",
       "              \n",
       "                      [[ 7.1481e-04, -9.6734e-03,  6.9475e-03, -1.8405e-02],\n",
       "                       [-4.4030e-03,  2.2664e-03, -1.0037e-02, -1.0667e-02],\n",
       "                       [ 7.7660e-03, -9.2348e-04, -3.6140e-03, -1.0011e-02],\n",
       "                       ...,\n",
       "                       [-9.9563e-03, -1.3490e-02, -1.4324e-02, -1.2173e-02],\n",
       "                       [-6.8196e-03, -1.4727e-04, -4.5959e-03, -1.4172e-02],\n",
       "                       [-1.1183e-02, -8.1083e-03,  5.3770e-03,  3.5849e-03]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[ 1.7413e-02, -3.5686e-02,  1.8460e-01,  9.3105e-02],\n",
       "                       [ 1.1545e-01,  1.8302e-01,  1.9561e-01, -2.2192e-02],\n",
       "                       [ 6.3997e-02,  3.3055e-01,  1.4739e-01,  2.4254e-01],\n",
       "                       ...,\n",
       "                       [ 6.7917e-03, -4.1831e-01,  1.6908e-01, -1.6338e-02],\n",
       "                       [ 3.1136e-01, -3.7795e-01,  1.2308e-01, -4.5851e-02],\n",
       "                       [-4.6348e-02,  1.3400e-01,  9.4808e-02, -1.7840e-01]],\n",
       "              \n",
       "                      [[ 9.0245e-02,  7.4608e-02,  7.6748e-02,  1.9850e-01],\n",
       "                       [ 7.7326e-02, -8.7576e-02, -7.1888e-02, -9.1000e-03],\n",
       "                       [ 6.5691e-02,  1.8001e-01,  3.4061e-01, -2.1792e-01],\n",
       "                       ...,\n",
       "                       [-4.5143e-02,  1.4977e-01,  1.8864e-01, -1.6382e-01],\n",
       "                       [-8.6780e-02,  3.5297e-02,  1.3511e-01, -8.0905e-02],\n",
       "                       [-1.5174e-01,  7.5970e-02, -1.4135e-01, -4.0038e-01]],\n",
       "              \n",
       "                      [[ 1.9366e-02,  7.6208e-02,  2.5060e-03, -1.1450e-02],\n",
       "                       [ 8.6636e-02, -2.1248e-01,  2.2349e-02, -1.1396e-01],\n",
       "                       [-9.5502e-04,  1.6268e-01,  7.2052e-02,  1.6471e-01],\n",
       "                       ...,\n",
       "                       [ 1.1110e-01, -2.9110e-02, -6.7505e-02,  7.1212e-02],\n",
       "                       [-9.3080e-02, -1.9069e-03,  9.9210e-03, -1.6634e-01],\n",
       "                       [-3.0642e-02, -8.3847e-02,  2.3585e-02, -4.1601e-02]]],\n",
       "                     device='cuda:0')),\n",
       "             ('conv2.bias',\n",
       "              tensor([-0.2701, -0.3967, -0.0146, -0.0097, -0.2186, -0.1886, -0.2702, -0.4373,\n",
       "                      -0.4435, -0.3182, -0.4503, -0.1465, -0.3247, -0.0225, -0.1063, -0.0093,\n",
       "                      -0.1700, -0.2246, -0.4449, -0.0828, -0.0505, -0.1947, -0.2346, -0.2928,\n",
       "                      -0.2542, -0.3506, -0.1220, -0.2878, -0.0577, -0.1777, -0.4203, -0.2496,\n",
       "                      -0.3246, -0.1336, -0.0166, -0.2640, -0.3822, -0.2774, -0.1466, -0.4630,\n",
       "                      -0.3768, -0.1686, -0.2097, -0.0987, -0.0249, -0.1593, -0.1073, -0.1916,\n",
       "                      -0.2698, -0.3086, -0.3026, -0.1731, -0.1985, -0.2886, -0.0102, -0.5346,\n",
       "                      -0.1704, -0.2821, -0.3078, -0.1588, -0.2396, -0.3890, -0.2732, -0.0191,\n",
       "                      -0.1512, -0.2872, -0.1943, -0.2106, -0.4367, -0.2154, -0.4411, -0.1196,\n",
       "                      -0.0138, -0.2948, -0.3800, -0.2908, -0.2659, -0.1272, -0.0610, -0.0202,\n",
       "                      -0.3412, -0.0273, -0.1381, -0.3458, -0.1787, -0.1517, -0.3086, -0.2811,\n",
       "                      -0.3269, -0.1553, -0.0150, -0.3926, -0.3763, -0.2254, -0.4598, -0.1012,\n",
       "                      -0.1751, -0.1777, -0.1747, -0.1352, -0.4463, -0.2907, -0.2597, -0.3245,\n",
       "                      -0.2556, -0.4460, -0.0591, -0.1323, -0.3365, -0.2536, -0.2305, -0.1478,\n",
       "                      -0.2381, -0.2536, -0.2368, -0.2150, -0.2375, -0.1223, -0.2058, -0.0369,\n",
       "                      -0.2600, -0.2635, -0.0174, -0.4061, -0.2679, -0.3082, -0.2725, -0.2489,\n",
       "                      -0.3055, -0.3364, -0.3366, -0.2257, -0.2737, -0.2006, -0.1225, -0.1201,\n",
       "                      -0.1958, -0.1631, -0.1973, -0.2529, -0.2360, -0.2173, -0.3707, -0.2832,\n",
       "                      -0.2283, -0.2823, -0.3048, -0.1619, -0.0022, -0.2538, -0.2859, -0.1950,\n",
       "                      -0.2282, -0.3050, -0.0157, -0.4066, -0.1080, -0.2703, -0.0938, -0.3255,\n",
       "                      -0.2200, -0.1842, -0.2017, -0.1882, -0.1743, -0.1308, -0.2892, -0.1765,\n",
       "                      -0.2437, -0.0560, -0.1886, -0.0322, -0.2948, -0.1048, -0.1737, -0.2706,\n",
       "                      -0.2230, -0.4616, -0.2224, -0.2494, -0.0469, -0.1627, -0.3664, -0.1657,\n",
       "                      -0.0857, -0.2766, -0.0518, -0.1907, -0.3049, -0.1728, -0.3780, -0.0534,\n",
       "                      -0.3032, -0.1731, -0.4761, -0.1878, -0.2896, -0.1845, -0.1671, -0.2696],\n",
       "                     device='cuda:0')),\n",
       "             ('conv3.weight',\n",
       "              tensor([[[ 0.0103,  0.0107, -0.0036, -0.0272, -0.0096],\n",
       "                       [ 0.0267,  0.0375, -0.0103,  0.0258,  0.0132],\n",
       "                       [ 0.0458,  0.0418,  0.0383,  0.0313,  0.0211],\n",
       "                       ...,\n",
       "                       [-0.0100,  0.0038, -0.0107,  0.0135,  0.0110],\n",
       "                       [ 0.0067, -0.0125, -0.0247,  0.0024,  0.0276],\n",
       "                       [-0.0052,  0.0173, -0.0150, -0.0148, -0.0156]],\n",
       "              \n",
       "                      [[ 0.0858,  0.1321, -0.0418,  0.1282, -0.0353],\n",
       "                       [-0.0067,  0.1623, -0.2399,  0.0340,  0.2072],\n",
       "                       [-0.1347,  0.2513,  0.6558,  0.0817, -0.1768],\n",
       "                       ...,\n",
       "                       [ 0.0467,  0.0289, -0.0448, -0.2652, -0.0187],\n",
       "                       [-0.2411,  0.1046,  0.4315, -0.0358,  0.0404],\n",
       "                       [ 0.1605,  0.0473,  0.0729, -0.0102,  0.1349]],\n",
       "              \n",
       "                      [[ 0.1529, -0.1055,  0.1183,  0.2054, -0.0689],\n",
       "                       [-0.0646,  0.2909,  0.4298, -0.0013,  0.0219],\n",
       "                       [-0.1438,  0.1404, -0.0982,  0.1073, -0.0077],\n",
       "                       ...,\n",
       "                       [-0.1753, -0.1845,  0.0397, -0.0189, -0.5748],\n",
       "                       [-0.0500,  0.4786,  0.0490, -0.0183, -0.1798],\n",
       "                       [-0.1777,  0.1769, -0.0675,  0.0733, -0.0606]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[ 0.0570,  0.2392,  0.0488,  0.2339,  0.0576],\n",
       "                       [-0.0518,  0.3763, -0.0338,  0.0015, -0.0601],\n",
       "                       [-0.1410,  0.0708,  0.6190,  0.2314,  0.4311],\n",
       "                       ...,\n",
       "                       [-0.0760, -0.0996,  0.0834, -0.2238, -0.1564],\n",
       "                       [ 0.2162, -0.1526, -0.5289, -0.1408, -0.0489],\n",
       "                       [ 0.0835,  0.2230, -0.1089, -0.1253,  0.0487]],\n",
       "              \n",
       "                      [[ 0.0660,  0.1000,  0.0787,  0.0864,  0.0321],\n",
       "                       [ 0.0352,  0.5695, -0.0714,  0.0488, -0.0104],\n",
       "                       [ 0.1660,  0.0714,  0.0238,  0.4576,  0.2072],\n",
       "                       ...,\n",
       "                       [-0.1153,  0.2310, -0.1858,  0.1271, -0.0914],\n",
       "                       [ 0.0424,  0.0933,  0.3058,  0.5560, -0.1601],\n",
       "                       [ 0.0024, -0.2161,  0.0013,  0.2600,  0.0773]],\n",
       "              \n",
       "                      [[ 0.1429,  0.0619,  0.0283, -0.0348, -0.0174],\n",
       "                       [ 0.0152, -0.0842,  0.1413, -0.0485,  0.1927],\n",
       "                       [ 0.3581, -0.2207, -0.0726, -0.0536,  0.1402],\n",
       "                       ...,\n",
       "                       [-0.1247, -0.2453, -0.2387, -0.1521,  0.1599],\n",
       "                       [-0.0462,  0.0263, -0.3253,  0.0724, -0.0558],\n",
       "                       [ 0.3959, -0.0643,  0.0283, -0.0397, -0.1224]]], device='cuda:0')),\n",
       "             ('conv3.bias',\n",
       "              tensor([-0.0318, -0.1811, -0.3230, -0.1206, -0.2857, -0.3746, -0.2725, -0.3458,\n",
       "                      -0.2298, -0.0861, -0.0853, -0.2632, -0.3021, -0.1237, -0.0342, -0.2899,\n",
       "                      -0.3630, -0.1304, -0.2568, -0.2516, -0.3376, -0.2787, -0.2083, -0.2564,\n",
       "                      -0.2290, -0.1251, -0.2844, -0.0188, -0.0383, -0.1323, -0.4415, -0.1639,\n",
       "                      -0.1628, -0.3936, -0.2790, -0.1791, -0.1757, -0.2837, -0.4166, -0.1499,\n",
       "                      -0.3245, -0.0510, -0.1715, -0.3817, -0.2615, -0.1564, -0.1471, -0.2008,\n",
       "                      -0.1622, -0.3769, -0.1397, -0.1497, -0.2667, -0.1647, -0.2832, -0.0342,\n",
       "                      -0.4503, -0.2071, -0.1383, -0.1767, -0.2678, -0.0415, -0.2921, -0.0935,\n",
       "                      -0.4124, -0.0974, -0.4122, -0.0975, -0.0196, -0.3096, -0.3975, -0.1658,\n",
       "                      -0.4259, -0.3001, -0.3165, -0.0222, -0.1003, -0.1461, -0.2031, -0.2438,\n",
       "                      -0.1362, -0.1503, -0.2957, -0.2667, -0.1243, -0.3274, -0.1488, -0.3871,\n",
       "                      -0.4274, -0.2887, -0.0443, -0.2964, -0.1045, -0.0286, -0.0295, -0.0119,\n",
       "                      -0.2584, -0.3044, -0.1000, -0.0261, -0.1484, -0.3288, -0.2602, -0.1021,\n",
       "                      -0.1852, -0.1414, -0.1216, -0.2359, -0.1724, -0.1548, -0.1473, -0.1314,\n",
       "                      -0.1889, -0.3959, -0.1103, -0.1109, -0.1762, -0.0103, -0.1319, -0.2272,\n",
       "                      -0.1535, -0.2722, -0.1472, -0.2842, -0.2730, -0.3253, -0.1497, -0.0676,\n",
       "                      -0.2810, -0.1398, -0.2691, -0.2834, -0.1540, -0.2600, -0.2079, -0.3633,\n",
       "                      -0.3532, -0.1597, -0.1704, -0.3815, -0.4117, -0.2557, -0.1610, -0.1652,\n",
       "                      -0.1421, -0.1654, -0.1547, -0.3769, -0.1301, -0.1201, -0.1298, -0.1282,\n",
       "                      -0.4344, -0.2489, -0.1361, -0.1494, -0.2217, -0.0720, -0.1403, -0.2740,\n",
       "                      -0.1500, -0.1371, -0.0408, -0.1290, -0.1341, -0.3005, -0.3942, -0.0632,\n",
       "                      -0.1668, -0.0916, -0.1419, -0.2633, -0.1713, -0.3524, -0.2821, -0.1556,\n",
       "                      -0.0207, -0.1460, -0.1076, -0.3383, -0.5121, -0.2413, -0.2391, -0.3199,\n",
       "                      -0.3956, -0.3058, -0.1193, -0.2628, -0.0112, -0.0210, -0.3984, -0.3166,\n",
       "                      -0.1912, -0.1547, -0.3829, -0.1376, -0.1617, -0.3083, -0.2726, -0.0956],\n",
       "                     device='cuda:0')),\n",
       "             ('fc1.weight',\n",
       "              tensor([[-0.1060,  0.4000, -0.0299,  ..., -0.1104, -0.0524, -0.1975],\n",
       "                      [-0.0271,  0.2024, -0.1549,  ...,  0.1477,  0.0394,  0.1018],\n",
       "                      [ 0.0439,  0.0479, -0.0080,  ...,  0.2115, -0.2992, -0.0717],\n",
       "                      ...,\n",
       "                      [-0.2223, -0.0386, -0.0372,  ...,  0.1386,  0.0221, -0.3523],\n",
       "                      [-0.0698,  0.0761, -0.0712,  ...,  0.1699, -0.2037, -0.1575],\n",
       "                      [-0.2421,  0.0550, -0.0497,  ...,  0.0536,  0.1193,  0.1342]],\n",
       "                     device='cuda:0')),\n",
       "             ('fc1.bias',\n",
       "              tensor([-0.1390, -0.2658, -0.2941, -0.1397, -0.1741,  0.2785, -0.1111,  0.1629,\n",
       "                      -0.4274, -0.3463, -0.2926, -0.1063, -0.0461,  0.0271, -0.1320, -0.3959,\n",
       "                      -0.0888,  0.4688,  0.3657, -0.2321, -0.7084, -0.3092, -0.2363, -0.3928,\n",
       "                      -0.1591,  0.6979, -0.2084, -0.3882, -0.2787,  0.4781,  0.0444, -0.2062,\n",
       "                      -0.1558, -0.1475, -0.1712, -0.4719,  0.1956, -0.3471, -0.3148, -0.3767,\n",
       "                      -0.1906,  0.1588, -0.2095, -0.2799,  0.2421, -0.1807,  0.5281, -0.1370,\n",
       "                      -0.0934,  0.3058, -0.1100, -0.1661, -0.0949,  0.0052,  0.0360, -0.2647,\n",
       "                       0.1288, -0.0134,  0.0111,  0.0937, -0.2432,  0.0635, -0.3159, -0.0820,\n",
       "                      -0.3408, -0.0868,  0.1794, -0.0106, -0.1674, -0.4255,  0.4442,  0.1330,\n",
       "                      -0.3051, -0.5308,  0.4876,  0.3989, -0.2314, -0.3309, -0.4195,  0.2599,\n",
       "                      -0.2005, -0.2077, -0.0963, -0.2384, -0.2902, -0.3937, -0.3374,  0.3297,\n",
       "                      -0.1418, -0.2920, -0.4358, -0.0865, -0.0268,  0.0924, -0.3474, -0.4532,\n",
       "                       0.3448, -0.4603,  0.4752, -0.0078, -0.4039, -0.3074, -0.4266, -0.4195,\n",
       "                      -0.4194, -0.1210,  0.4559, -0.2108, -0.1512,  0.1592, -0.2431, -0.2449,\n",
       "                      -0.1860, -0.5312, -0.0189, -0.1816, -0.5620, -0.0185,  0.1438, -0.0884,\n",
       "                      -0.1172, -0.0361, -0.1973, -0.2720,  0.0043, -0.1500, -0.5133,  0.0858,\n",
       "                      -0.1659, -0.5816,  0.0550,  0.4573, -0.2049,  0.3618, -0.4559,  0.0306,\n",
       "                      -0.3536, -0.5269, -0.2975, -0.3061,  0.4433,  0.5074, -0.1281,  0.4029,\n",
       "                      -0.2563, -0.1742, -0.2812,  0.0597,  0.1783, -0.6529, -0.0973, -0.2544,\n",
       "                       0.0299, -0.5035,  0.0368, -0.3323,  0.0902, -0.6864,  0.1453, -0.1352,\n",
       "                       0.1861, -0.3981, -0.2412,  0.1387, -0.4792, -0.0438, -0.4936, -0.3879,\n",
       "                      -0.1449,  0.2257,  0.0120, -0.2326,  0.1829,  0.7605, -0.1552,  0.1965,\n",
       "                      -0.3074,  0.0359, -0.5058,  0.4726, -0.2182, -0.0802, -0.0951, -0.2123,\n",
       "                       0.5282, -0.1014,  0.5942,  0.2923, -0.3098,  0.5371,  0.2766, -0.5307,\n",
       "                      -0.3013, -0.1750, -0.0301, -0.4019, -0.3747, -0.1966, -0.4531, -0.4599,\n",
       "                      -0.5881,  0.2047, -0.1287,  0.1666,  0.1084, -0.3207, -0.2321, -0.1349,\n",
       "                      -0.3508, -0.4301,  0.3837, -0.3858, -0.3702, -0.0438, -0.2603,  0.4136,\n",
       "                      -0.4729, -0.2890, -0.1722, -0.4304, -0.4272,  0.5330, -0.2788,  0.4112,\n",
       "                      -0.5134, -0.3454,  0.3036, -0.2509,  0.3751, -0.2905, -0.1338, -0.1509,\n",
       "                      -0.5307,  0.0055, -0.5178, -0.1807, -0.4715, -0.1892,  0.0860, -0.1060,\n",
       "                       0.4674, -0.4344, -0.4176, -0.3128,  0.6096, -0.1617,  0.2625, -0.1859,\n",
       "                       0.4745, -0.1674,  0.4741,  0.1784, -0.1067, -0.0325, -0.0714, -0.5077],\n",
       "                     device='cuda:0')),\n",
       "             ('fc2.weight',\n",
       "              tensor([[ 0.1082,  0.0740,  0.2463,  ..., -0.1151, -0.1707, -0.0563],\n",
       "                      [-0.1688,  0.0456,  0.0361,  ...,  0.0256, -0.1666,  0.0845],\n",
       "                      [-0.0479, -0.0397,  0.0186,  ..., -0.0476,  0.0311,  0.0409],\n",
       "                      ...,\n",
       "                      [-0.0125,  0.0147, -0.1189,  ..., -0.0063,  0.1340, -0.2751],\n",
       "                      [ 0.0369,  0.0059, -0.0506,  ..., -0.0659, -0.0341, -0.0514],\n",
       "                      [ 0.0756,  0.0950, -0.0647,  ..., -0.0725,  0.4006,  0.1267]],\n",
       "                     device='cuda:0')),\n",
       "             ('fc2.bias',\n",
       "              tensor([ 1.0557e-01,  1.5843e-01, -2.3401e-02,  4.4364e-01, -6.4229e-02,\n",
       "                      -6.2494e-02, -5.6424e-02, -5.4680e-02, -2.8233e-02, -4.5645e-02,\n",
       "                       1.1358e-02, -7.5241e-02,  6.7924e-02,  8.9167e-02,  1.2580e-01,\n",
       "                      -3.3050e-02, -1.2574e-01,  4.1374e-02, -6.2315e-02,  2.4073e-01,\n",
       "                      -5.1998e-01, -6.1221e-02, -7.1692e-02,  7.9684e-01, -4.7869e-02,\n",
       "                      -4.5623e-02,  2.3935e-02,  2.8973e-01,  7.4367e-03,  4.3390e-03,\n",
       "                      -9.0943e-02,  3.1709e-01, -1.0396e-01,  1.6910e-01, -6.0984e-02,\n",
       "                      -1.0258e-02,  2.6709e-01,  2.9956e-01, -1.9975e-01,  4.6031e-01,\n",
       "                       1.5662e-02, -2.5626e-02,  3.6781e-03,  1.3257e-01,  1.6219e-02,\n",
       "                       2.3230e-01,  2.0313e-01,  8.8414e-02,  9.4965e-02,  3.9346e-02,\n",
       "                       4.3639e-01, -8.6584e-02,  6.9492e-02, -6.3625e-02,  8.5660e-02,\n",
       "                      -2.6469e-01, -1.4861e-01,  1.3863e-01,  4.1267e-02, -6.3715e-02,\n",
       "                       5.0986e-01,  4.0972e-03,  7.3860e-01,  3.6734e-01,  1.7333e-01,\n",
       "                      -2.5762e-01, -3.9999e-02, -1.5052e-01, -1.3843e-01,  2.0249e-01,\n",
       "                      -9.6534e-03, -5.0266e-02, -1.2620e-02, -1.1515e-02, -9.2030e-02,\n",
       "                       6.9924e-01, -1.7220e-01, -5.9242e-01, -6.0667e-02,  1.1965e-01,\n",
       "                      -7.4599e-02,  1.3407e-04, -1.7596e-01, -1.7527e-01, -1.2379e-01,\n",
       "                       2.2110e-01, -6.9870e-02,  7.1556e-01, -3.6320e-02,  3.9337e-01,\n",
       "                      -5.6320e-02, -1.2082e-01, -2.1108e-01, -2.6407e-01, -9.0335e-02,\n",
       "                      -2.6403e-01, -8.8149e-02,  3.7201e-01,  4.4656e-01,  2.1423e-02,\n",
       "                      -1.0446e-01,  4.8340e-02,  2.6213e-01,  5.4811e-01, -1.2187e-01,\n",
       "                      -6.0145e-02,  5.8577e-01,  3.8211e-02, -1.4887e-01,  2.7342e-01,\n",
       "                      -1.7040e-01,  2.0086e-01,  1.0929e-01, -6.9638e-02,  7.9171e-01,\n",
       "                       4.1246e-01, -2.2391e-01, -3.5767e-02,  3.0168e-01,  4.9841e-01,\n",
       "                      -1.4842e-01, -7.1044e-02, -8.8315e-02,  4.3898e-01,  2.4114e-01,\n",
       "                       5.6820e-02, -2.7957e-02,  5.7092e-01], device='cuda:0')),\n",
       "             ('fc3.weight',\n",
       "              tensor([[-0.0102, -0.0883,  0.0387,  0.1419, -0.0679, -0.0514, -0.0365,  0.0687,\n",
       "                        0.0251,  0.0150,  0.0227, -0.0147, -0.0829,  0.0587, -0.0807, -0.0633,\n",
       "                        0.0086, -0.0113, -0.0225,  0.0234, -0.0654,  0.0789, -0.0827,  0.1939,\n",
       "                       -0.0401,  0.0529,  0.0192,  0.0315, -0.0571,  0.0474, -0.0052, -0.0648,\n",
       "                       -0.0396, -0.0568, -0.0614, -0.0278, -0.1029,  0.0566, -0.0388, -0.0690,\n",
       "                        0.0475, -0.0131, -0.0178,  0.0182, -0.0588, -0.0279,  0.0099, -0.0567,\n",
       "                       -0.0641, -0.0473,  0.0553, -0.0394, -0.1470, -0.0061, -0.0848, -0.0483,\n",
       "                        0.0636,  0.0491, -0.0810, -0.1185, -0.1127, -0.0405,  0.2070,  0.0041,\n",
       "                        0.1102,  0.0733, -0.0201, -0.0572,  0.0046,  0.0562,  0.0197, -0.0470,\n",
       "                       -0.0158, -0.0414, -0.0218,  0.1626,  0.0225, -0.0553,  0.0005,  0.0886,\n",
       "                       -0.0193, -0.0226, -0.0201,  0.0256, -0.0478, -0.0356,  0.0085,  0.0965,\n",
       "                       -0.0526,  0.1038, -0.0533, -0.0736,  0.0463, -0.0457, -0.0337, -0.0422,\n",
       "                       -0.0334, -0.0684, -0.1333,  0.0213,  0.0719, -0.0012, -0.0258, -0.1328,\n",
       "                        0.0386, -0.0441,  0.0391,  0.0711, -0.0773, -0.0302, -0.0233,  0.0333,\n",
       "                       -0.0116,  0.0095,  0.1842, -0.0731,  0.0175, -0.1136, -0.0024, -0.0540,\n",
       "                       -0.0577, -0.0224,  0.0068, -0.1721, -0.0015, -0.0038,  0.0105,  0.1135],\n",
       "                      [-0.0059,  0.0809,  0.0420, -0.2032, -0.0563, -0.0544, -0.0053,  0.0368,\n",
       "                        0.0473,  0.0272,  0.0378, -0.0250,  0.0987, -0.0910, -0.0309,  0.1105,\n",
       "                        0.0461, -0.0096, -0.0282, -0.0075,  0.0042,  0.0571, -0.0663, -0.1740,\n",
       "                        0.0387,  0.0560,  0.0993,  0.0061,  0.0600,  0.0395,  0.0027, -0.0266,\n",
       "                       -0.0462, -0.0517, -0.0386,  0.0499,  0.0767, -0.0299, -0.0184,  0.0679,\n",
       "                        0.0596,  0.0401, -0.0119,  0.0413, -0.0347, -0.0227,  0.0183,  0.0246,\n",
       "                       -0.0681, -0.1047, -0.0941, -0.0485,  0.1392,  0.0087, -0.0550,  0.0058,\n",
       "                        0.0700,  0.0630,  0.0117,  0.1056,  0.1260, -0.0200, -0.1019,  0.0243,\n",
       "                       -0.0602, -0.1927, -0.0516, -0.0931,  0.0063,  0.0158,  0.0037,  0.0025,\n",
       "                        0.0174, -0.0618,  0.0018, -0.1496, -0.0051,  0.0156, -0.0209,  0.0702,\n",
       "                       -0.0338,  0.0175,  0.0597,  0.0409, -0.0130, -0.0411, -0.0036, -0.1919,\n",
       "                        0.0041, -0.1013,  0.0128, -0.0405,  0.1016, -0.0606, -0.0482, -0.0065,\n",
       "                       -0.0122,  0.0531,  0.0563,  0.0782,  0.0778,  0.0319, -0.0215,  0.0791,\n",
       "                        0.0104, -0.0326, -0.0913,  0.0620, -0.0627,  0.0957,  0.0855,  0.0543,\n",
       "                        0.0065,  0.0214, -0.1783,  0.0471,  0.0674, -0.0251, -0.0787,  0.0637,\n",
       "                       -0.0672,  0.0291, -0.0044,  0.0741,  0.0135,  0.0550,  0.0169, -0.1533]],\n",
       "                     device='cuda:0')),\n",
       "             ('fc3.bias', tensor([ 0.2420, -0.1828], device='cuda:0'))])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing started.......\n",
      "Confusion matrix: \n",
      " [[27 18]\n",
      " [27 18]]\n",
      "\n",
      "TP: 18\n",
      "FP: 18\n",
      "TN: 27\n",
      "FN: 27\n",
      "\n",
      "Accuracy: 0.5\n",
      "Precision: 0.5\n",
      "F-measure: 0.4444444444444445\n",
      "Recall: 0.4\n",
      "Precision-Recall AUC: 0.5137922521610677\n",
      "AUC: 0.5069135802469136\n",
      "MCC: 0.0\n"
     ]
    }
   ],
   "source": [
    "print('Testing started.......')\n",
    "## Testing\n",
    "checkpoint = torch.load('models/VB-CNN_devign/model_ep_17.tar', map_location='cuda')\n",
    "model.load_state_dict(new_dict)\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    running_acc_test = 0\n",
    "    running_loss_test = 0\n",
    "    all_pred=[]\n",
    "    all_labels=[]\n",
    "    for batch in test_iterator:\n",
    "        batch.codes, batch.label = batch.codes.to(device), batch.label.to(device)\n",
    "        output_test = model(batch.codes).squeeze(1)\n",
    "        loss_test = criterion(output_test,batch.label)\n",
    "        acc_test = softmax_accuracy(output_test,batch.label)\n",
    "        running_acc_test += acc_test\n",
    "        running_loss_test += loss_test.item()\n",
    "        all_pred += output_test.tolist()\n",
    "        all_labels += batch.label.tolist()\n",
    "\n",
    "ap=evaluate_testing(all_pred, all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use below only on MVD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn=['non-vulnerable','CWE-404','CWE-476','CWE-119','CWE-706','CWE-670','CWE-673','CWE-119, CWE-666, CWE-573','CWE-573','CWE-668','CWE-400, CWE-665, CWE-020','CWE-662','CWE-400','CWE-665','CWE-020','CWE-074','CWE-362','CWE-191','CWE-190','CWE-610','CWE-704','CWE-170','CWE-676','CWE-187','CWE-138','CWE-369','CWE-662, CWE-573','CWE-834','CWE-400, CWE-665','CWE-400, CWE-404','CWE-221','CWE-754','CWE-311','CWE-404, CWE-668','CWE-506','CWE-758','CWE-666','CWE-467','CWE-327','CWE-666, CWE-573','CWE-469']\n",
    "report = sklearn.metrics.classification_report(y_true=all_labels, y_pred=ap, digits=6,labels=np.arange(0,41),target_names=tn)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = sklearn.metrics.confusion_matrix(y_true=[1 if x == 0 else 0 for x in all_labels], y_pred=[1 if x == 0 else 0 for x in ap])\n",
    "tn, fp, fn, tp = confusion.ravel()\n",
    "print('\\nTP:',tp)\n",
    "print('FP:',fp)\n",
    "print('TN:',tn)\n",
    "print('FN:',fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fpr = []\n",
    "w_all_fpr = []\n",
    "aug_y_true_sum = 0\n",
    "for counter in range(41):\n",
    "    aug_y_true = [1 if x == counter else 0 for x in all_labels]\n",
    "    aug_y_pred = [1 if x == counter else 0 for x in ap]\n",
    "    confusion = sklearn.metrics.confusion_matrix(y_true=aug_y_true, y_pred=aug_y_pred)\n",
    "    tn, fp, fn, tp = confusion.ravel()\n",
    "    all_fpr.append(fp/(fp+tn))  ## FPR\n",
    "    w_all_fpr.append((fp/(fp+tn))*aug_y_true.count(1))  ## w_FPR\n",
    "    aug_y_true_sum += aug_y_true.count(1)\n",
    "\n",
    "print('FPR: ', sum(all_fpr)/41.0*100.0)\n",
    "print('Weighted FPR: ', sum(w_all_fpr)/aug_y_true_sum*100.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
